# ğŸ­ Facial Emotion Recognition using CNN + MobileNetV2

This project detects **human emotions in real-time** using a webcam or IP camera feed.  
It uses **TensorFlow**, **OpenCV**, and **MediaPipe** to identify faces and classify emotions such as *Happy, Sad, Angry, Neutral, Fear, Disgust,* and *Surprise*.

---

## ğŸš€ Features
- ğŸ§  **Deep Learning (CNN with MobileNetV2)** trained on FER2013 dataset  
- ğŸ¥ **Real-time detection** using webcam or mobile camera (IP Webcam app)  
- ğŸ§© **MediaPipe Face Detection** for accurate face tracking  
- âš™ï¸ **TensorFlow model** trained on grayscale images  
- ğŸ“Š **Smooth predictions** using averaged probability buffer  

---

## ğŸ—‚ï¸ Project Structure
```
Facial-Emotion-Recognition/
â”œâ”€â”€ train_emotion_cnn.py       # Model training script
â”œâ”€â”€ realtime_emotion.py        # Real-time emotion detection
â”œâ”€â”€ fer_MobileNet2.keras       # Saved trained model
â”œâ”€â”€ labels.json                # Emotion label mappings
â””â”€â”€ README.md                  # Project documentation
```

---

## âš™ï¸ Requirements
Install the dependencies using `pip`:

```bash
pip install tensorflow opencv-python mediapipe numpy matplotlib scikit-learn
```

---

## ğŸ§  Training the Model
To train the model from scratch:
1. Download and prepare the **FER2013 dataset**.
   ```
   dataset/
   â”œâ”€â”€ train/
   â””â”€â”€ test/
   ```
2. Run the training script:
   ```bash
   python train_emotion_cnn.py
   ```
3. This will generate:
   - `fer_MobileNetV2_gray.keras` â†’ trained model  
   - `labels.json` â†’ list of emotion labels  

---

## ğŸ¥ Running Real-Time Emotion Detection
Make sure your trained model and `labels.json` are in the same folder as `realtime_emotion.py`.

Then run:
```bash
python realtime_emotion.py
```

To use your **mobile camera**, replace this line in `realtime_emotion.py`:
```python
url = 0
```
with your IP Webcam URL, for example:
```python
url = "http://192.168.1.5:8080/video"
```

Press **`Q`** to quit the window.

---

## ğŸ§© Supported Emotions
| Label | Description |
|--------|-------------|
| ğŸ˜€ Happy | Smiling or joyful |
| ğŸ˜¢ Sad | Unhappy expression |
| ğŸ˜  Angry | Frowning or tense |
| ğŸ˜± Surprise | Wide eyes or raised brows |
| ğŸ˜ Neutral | Relaxed face |
| ğŸ˜¨ Fear | Anxious or tense expression |
| ğŸ¤¢ Disgust | Nose wrinkled or mouth open |

---

## ğŸ“ˆ Model Summary
The model is based on **MobileNetV2**, modified for grayscale input (48Ã—48).  
It uses data augmentation, dropout, and batch normalization to improve accuracy and reduce overfitting.

---

## ğŸ† Results
- Achieved high accuracy on test set (~70â€“75% depending on training setup).  
- Works in real-time at 20â€“30 FPS with a CPU or GPU.

---

## ğŸ§‘â€ğŸ’» Author
**Rugved Ganapuram**  
ğŸ“§ rugvedg10@gmail.com  
ğŸŒ [GitHub Profile](https://github.com/Rugved-G)

---

## ğŸ“œ License
This project is open source under the [MIT License](LICENSE).

---

### â¤ï¸ Acknowledgments
- [TensorFlow](https://www.tensorflow.org/)
- [OpenCV](https://opencv.org/)
- [MediaPipe](https://developers.google.com/mediapipe)
- [FER2013 Dataset](https://www.kaggle.com/datasets/msambare/fer2013)
